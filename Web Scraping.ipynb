{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d40b032-5332-4633-9dc7-a65d4a2d4707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wasim\\miniconda3\\envs\\myenv\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests            # For making HTTP requests to fetch webpage content\n",
    "from bs4 import BeautifulSoup  # For parsing HTML and extracting data\n",
    "import pandas as pd        # For organizing data into dataframes and exporting\n",
    "import time               # Optional: to add delay between requests to avoid server overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ecf4207-9733-4e0a-adf2-ff295530b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flipkart product reviews URL (can be changed for different products)\n",
    "URL = \"https://www.flipkart.com/apple-iphone-15-blue-128-gb/product-reviews/itmbf14ef54f645d?pid=MOBGTAGPAQNVFZZY&lid=LSTMOBGTAGPAQNVFZZY7RHDU7&marketplace=FLIPKART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663b901-50e0-45c0-b878-156888571e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pages to scrape (e.g., 30 pages)\n",
    "NUM_PAGES = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774daf5-f5d4-4a97-9efa-3d2aae340a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the data\n",
    "Usernames = []\n",
    "Ratings = []\n",
    "Reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd67d50-1546-4610-ba29-2d7cca4cbd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1, NUM_PAGES + 1):\n",
    "    url = f\"{URL}&page={page}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page}, status code: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2c6a3-915c-402d-86b1-0b2f103bba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1, NUM_PAGES + 1):\n",
    "    url = f\"{URL}&page={page}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page}, status code: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract Usernames (Reviewers' names)\n",
    "    name_tags = soup.find_all(\"p\", class_=\"_2NsDsF AwS1CA\")\n",
    "    page_names = [tag.get_text(strip=True) for tag in name_tags]\n",
    "    Usernames.extend(page_names)\n",
    "\n",
    "    # Extract Ratings (star ratings for reviews)\n",
    "    rating_tags = soup.find_all(\"div\", class_=\"XQDdHH Ga3i8K\")\n",
    "    page_ratings = [tag.get_text(strip=True) for tag in rating_tags]\n",
    "    Ratings.extend(page_ratings)\n",
    "\n",
    "    # Extract Reviews (main review text)\n",
    "    review_tags = soup.find_all(\"div\", class_=\"ZmyHeo\")\n",
    "    page_reviews = [tag.get_text(strip=True) for tag in review_tags]\n",
    "    Reviews.extend(page_reviews)\n",
    "\n",
    "    # Optional: delay to mimic human browsing and be respectful\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15deabd6-65ea-4b94-959e-fa9722bb9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA CLEANUP ---\n",
    "\n",
    "# To be safe, match lengths across all lists\n",
    "min_len = min(len(Usernames), len(Ratings), len(Reviews))\n",
    "Usernames = Usernames[:min_len]\n",
    "Ratings = Ratings[:min_len]\n",
    "Reviews = Reviews[:min_len]\n",
    "\n",
    "# --- ORGANIZE AND EXPORT DATA ---\n",
    "\n",
    "# Build DataFrame for easy analysis/export\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": Usernames,\n",
    "    \"Rating\": Ratings,\n",
    "    \"Review\": Reviews\n",
    "})\n",
    "\n",
    "# EXAMPLE: Print first 5 reviews\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Save to CSV for further analysis\n",
    "df.to_csv(\"iphone15_flipkart_reviews.csv\", index=False)\n",
    "print(\"Done! Data saved to iphone15_flipkart_reviews.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
